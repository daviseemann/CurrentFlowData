{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2 as pg\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "\n",
    "\n",
    "def db_connect():\n",
    "    try:\n",
    "        conn = pg.connect(\n",
    "            dbname=\"current_flow_db\",\n",
    "            user=\"postgres\",\n",
    "            password=\"admin321\",\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"I am unable to connect to the database\")\n",
    "        print(e)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = db_connect()\n",
    "cursor = db_connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'carga_diaria' created successfully\n",
      "Table 'Etags' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create tables in the database: carga_diaria and Etags.\n",
    "class CreateTables:\n",
    "    def __init__(self, connect_pg):\n",
    "        self.connect_pg = connect_pg\n",
    "        self.cursor = connect_pg.cursor()\n",
    "\n",
    "    def carga_diaria(self):\n",
    "        self.cursor.execute(\n",
    "            \"\"\"CREATE TABLE IF NOT EXISTS carga_diaria (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    id_subsistema VARCHAR NOT NULL,\n",
    "                    nom_subsistema VARCHAR NOT NULL,\n",
    "                    din_instante VARCHAR NOT NULL,\n",
    "                    val_cargaenergiamwmed VARCHAR,\n",
    "                    Ano INTEGER NOT NULL,\n",
    "                    input_file VARCHAR NOT NULL,\n",
    "                    CONSTRAINT unique_constraint_carga_diaria UNIQUE (id_subsistema, din_instante)\n",
    "            );\"\"\"\n",
    "        )\n",
    "        self.connect_pg.commit()\n",
    "        print(\"Table 'carga_diaria' created successfully\")\n",
    "\n",
    "    def Etags(self):\n",
    "        self.cursor.execute(\n",
    "            \"\"\"CREATE TABLE IF NOT EXISTS Etags (\n",
    "                URL TEXT PRIMARY KEY,\n",
    "                ETag TEXT\n",
    "            );\"\"\"\n",
    "        )\n",
    "        self.connect_pg.commit()\n",
    "        print(\"Table 'Etags' created successfully\")\n",
    "\n",
    "\n",
    "# Supondo que `connect_pg` seja uma conexão válida com o banco de dados PostgreSQL\n",
    "create_tables = CreateTables(connect_pg=db_connection)\n",
    "create_tables.carga_diaria()\n",
    "create_tables.Etags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging for better readability\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to INFO or DEBUG for more verbose output\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(stream=sys.stdout),  # Log to console\n",
    "        # logging.FileHandler(\"file_downloads.log\"),  # Log to a file\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Function to load ETags from the database\n",
    "def load_etags_db():\n",
    "    cursor.execute(\"SELECT * FROM Etags\")  # Assuming `cursor` is a connected DB cursor\n",
    "    etags = cursor.fetchall()\n",
    "    etags_df = pd.DataFrame(etags, columns=[\"URL\", \"ETag\"])\n",
    "    logging.info(\"Loaded ETags from the database.\")\n",
    "    return etags_df\n",
    "\n",
    "\n",
    "# Function to get the ETag from a URL\n",
    "def get_etag(url):\n",
    "    try:\n",
    "        logging.info(f\"Fetching ETag for {url}\")\n",
    "        response = requests.head(f\"{url}\")\n",
    "        response.raise_for_status()  # Ensure the request was successful\n",
    "        etag = response.headers.get(\"ETag\")\n",
    "        logging.info(f\"→ ETag fetched: {etag}\")\n",
    "        return etag\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Failed to fetch ETag for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to compare the current ETag with the stored ETag\n",
    "def compare_etag(url, new_etag):\n",
    "    previous_etag = etags_df[etags_df[\"URL\"] == url][\"ETag\"].values\n",
    "    return len(previous_etag) == 0 or previous_etag[0] != new_etag\n",
    "\n",
    "\n",
    "# Function to stage URLs for download if their ETag has changed\n",
    "def stage_etag(urls):\n",
    "    urls_to_update = []\n",
    "    logging.info(\"Checking for updates based on ETag comparison...\")\n",
    "    for url in urls:\n",
    "        etag = get_etag(url)\n",
    "        if etag and compare_etag(url, etag):\n",
    "            urls_to_update.append(url)\n",
    "            logging.info(f\"✔ URL staged for download: {url}\")\n",
    "        else:\n",
    "            logging.info(f\"✖ URL is up-to-date: {url}\")\n",
    "    return urls_to_update\n",
    "\n",
    "\n",
    "# Function to download the file from the URL and save it\n",
    "def download_file(url, save_dir=\"data/\"):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Starting download: {url}\")\n",
    "        response = requests.get(f\"{url}\")\n",
    "        response.raise_for_status()  # Ensure the request was successful\n",
    "        file_path = os.path.join(save_dir, os.path.basename(url))\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        logging.info(f\"→ Download complete: {url} saved to {file_path}\")\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "\n",
    "# Function to update the database with new ETags\n",
    "def update_etag_db(url, new_etag):\n",
    "    try:\n",
    "        logging.info(f\"Updating ETag for {url} in the database.\")\n",
    "        # Check if the URL already exists in the database\n",
    "        cursor.execute(\"SELECT * FROM Etags WHERE URL = %s\", (url,))\n",
    "        result = cursor.fetchone()\n",
    "\n",
    "        if result:\n",
    "            # If URL exists, update the ETag\n",
    "            cursor.execute(\"UPDATE Etags SET ETag = %s WHERE URL = %s\", (new_etag, url))\n",
    "        else:\n",
    "            # If URL does not exist, insert a new row\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO Etags (URL, ETag) VALUES (%s, %s)\", (url, new_etag)\n",
    "            )\n",
    "\n",
    "        # Commit the transaction to save changes\n",
    "        db_connection.commit()\n",
    "        logging.info(f\"→ Database updated: ETag for {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to update ETag for {url} in the database: {e}\")\n",
    "        db_connection.rollback()  # Rollback in case of any issues\n",
    "\n",
    "\n",
    "# Function to process URLs: compare ETags, download if necessary, and update the DataFrame and database\n",
    "def process_urls(urls, save_dir=\"data/\"):\n",
    "    global etags_df\n",
    "\n",
    "    # Step 1: Stage URLs that need updating\n",
    "    urls_to_update = stage_etag(urls)\n",
    "\n",
    "    if urls_to_update:\n",
    "        logging.info(f\"URLs to be updated: {len(urls_to_update)} files\")\n",
    "\n",
    "        # Step 2: Download the files for the staged URLs\n",
    "        for url in urls_to_update:\n",
    "            logging.info(f\"--- Processing {url} ---\")\n",
    "            download_file(url, save_dir)\n",
    "\n",
    "            # Update the DataFrame with the new ETag after downloading\n",
    "            new_etag = get_etag(url)\n",
    "            if new_etag:\n",
    "                if not etags_df[etags_df[\"URL\"] == url].empty:\n",
    "                    etags_df.loc[etags_df[\"URL\"] == url, \"ETag\"] = new_etag\n",
    "                else:\n",
    "                    etags_df.loc[len(etags_df)] = [url, new_etag]\n",
    "\n",
    "                # Step 3: Update the ETag in the database\n",
    "                update_etag_db(url, new_etag)\n",
    "\n",
    "        logging.info(\n",
    "            \"All URLs have been processed. ETag DataFrame and database updated.\"\n",
    "        )\n",
    "    else:\n",
    "        logging.info(\"No files need to be updated. All files are up-to-date.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL for the API\n",
    "url_base = \"https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/\"\n",
    "\n",
    "# Define the range of years for which we want to fetch data\n",
    "years = range(2015, 2025)\n",
    "\n",
    "# Generate the list of URLs for each year\n",
    "urls = [f\"{url_base}CARGA_ENERGIA_{year}.csv\" for year in years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-27 00:36:40,605 [INFO] Loaded ETags from the database.\n",
      "2024-09-27 00:36:40,607 [INFO] Checking for updates based on ETag comparison...\n",
      "2024-09-27 00:36:40,608 [INFO] Fetching ETag for https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2015.csv\n",
      "2024-09-27 00:36:42,413 [INFO] → ETag fetched: \"2ef2d1ac377178d14bfa59e64dc578e3\"\n",
      "2024-09-27 00:36:42,418 [INFO] ✖ URL is up-to-date: https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2015.csv\n",
      "2024-09-27 00:36:42,420 [INFO] Fetching ETag for https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2016.csv\n",
      "2024-09-27 00:36:43,608 [INFO] → ETag fetched: \"f2faf3fdd60276f2084235ea796f9d68\"\n",
      "2024-09-27 00:36:43,611 [INFO] ✖ URL is up-to-date: https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2016.csv\n",
      "2024-09-27 00:36:43,612 [INFO] Fetching ETag for https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2017.csv\n",
      "2024-09-27 00:36:44,706 [INFO] → ETag fetched: \"69e7eab6ba21de9eef150628b78c5e45\"\n",
      "2024-09-27 00:36:44,710 [INFO] ✖ URL is up-to-date: https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2017.csv\n",
      "2024-09-27 00:36:44,711 [INFO] Fetching ETag for https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2018.csv\n",
      "2024-09-27 00:36:45,774 [INFO] → ETag fetched: \"b3bf852dd993fccc37e9d639d6f518fa\"\n",
      "2024-09-27 00:36:45,778 [INFO] ✖ URL is up-to-date: https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2018.csv\n",
      "2024-09-27 00:36:45,781 [INFO] Fetching ETag for https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2019.csv\n",
      "2024-09-27 00:36:46,863 [INFO] → ETag fetched: \"97a15c5091ddd7b366255b772a789cfb\"\n",
      "2024-09-27 00:36:46,866 [INFO] ✖ URL is up-to-date: https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2019.csv\n",
      "2024-09-27 00:36:46,867 [INFO] Fetching ETag for https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2020.csv\n",
      "2024-09-27 00:36:47,993 [INFO] → ETag fetched: \"3fdb975298faa109c9a7008a786a2273\"\n",
      "2024-09-27 00:36:47,997 [INFO] ✖ URL is up-to-date: https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2020.csv\n",
      "2024-09-27 00:36:47,998 [INFO] Fetching ETag for https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2021.csv\n",
      "2024-09-27 00:36:49,075 [INFO] → ETag fetched: \"9336ea5ca0861a1a4a5df3f78c9eeed6\"\n",
      "2024-09-27 00:36:49,079 [INFO] ✖ URL is up-to-date: https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2021.csv\n",
      "2024-09-27 00:36:49,080 [INFO] Fetching ETag for https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2022.csv\n",
      "2024-09-27 00:36:50,135 [INFO] → ETag fetched: \"8923a92618dc772cf5938939a1c325c5\"\n",
      "2024-09-27 00:36:50,140 [INFO] ✖ URL is up-to-date: https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2022.csv\n",
      "2024-09-27 00:36:50,141 [INFO] Fetching ETag for https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2023.csv\n",
      "2024-09-27 00:36:51,255 [INFO] → ETag fetched: \"f8e686719288d7ab2bca04519c849a12\"\n",
      "2024-09-27 00:36:51,258 [INFO] ✖ URL is up-to-date: https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2023.csv\n",
      "2024-09-27 00:36:51,259 [INFO] Fetching ETag for https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2024.csv\n",
      "2024-09-27 00:36:52,417 [INFO] → ETag fetched: \"23ea89825c8a5a6d7b276cfe5fd1acce\"\n",
      "2024-09-27 00:36:52,421 [INFO] ✖ URL is up-to-date: https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/carga_energia_di/CARGA_ENERGIA_2024.csv\n",
      "2024-09-27 00:36:52,423 [INFO] No files need to be updated. All files are up-to-date.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DataFrame by loading ETags from the database\n",
    "etags_df = load_etags_db()\n",
    "\n",
    "# Example usage\n",
    "process_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-27 00:37:37,656 [INFO] Processing files in directory: ../data/\n",
      "2024-09-27 00:37:37,658 [INFO] Reading CSV file: ../data/CARGA_ENERGIA_2015.csv\n",
      "2024-09-27 00:37:37,670 [INFO] File ../data/CARGA_ENERGIA_2015.csv read successfully with 1460 rows.\n",
      "2024-09-27 00:37:37,673 [INFO] Loading data from CARGA_ENERGIA_2015.csv into the database.\n",
      "2024-09-27 00:37:37,674 [INFO] Deleting existing rows for CARGA_ENERGIA_2015.csv from the database.\n",
      "2024-09-27 00:37:37,689 [INFO] → Rows for CARGA_ENERGIA_2015.csv deleted successfully.\n",
      "2024-09-27 00:37:37,690 [INFO] Inserting new rows for CARGA_ENERGIA_2015.csv into the database.\n",
      "2024-09-27 00:37:38,953 [INFO] → Rows for CARGA_ENERGIA_2015.csv inserted successfully.\n",
      "2024-09-27 00:37:38,955 [INFO] Reading CSV file: ../data/CARGA_ENERGIA_2016.csv\n",
      "2024-09-27 00:37:38,960 [INFO] File ../data/CARGA_ENERGIA_2016.csv read successfully with 1464 rows.\n",
      "2024-09-27 00:37:38,962 [INFO] Loading data from CARGA_ENERGIA_2016.csv into the database.\n",
      "2024-09-27 00:37:38,963 [INFO] Deleting existing rows for CARGA_ENERGIA_2016.csv from the database.\n",
      "2024-09-27 00:37:38,974 [INFO] → Rows for CARGA_ENERGIA_2016.csv deleted successfully.\n",
      "2024-09-27 00:37:38,975 [INFO] Inserting new rows for CARGA_ENERGIA_2016.csv into the database.\n",
      "2024-09-27 00:37:40,112 [INFO] → Rows for CARGA_ENERGIA_2016.csv inserted successfully.\n",
      "2024-09-27 00:37:40,113 [INFO] Reading CSV file: ../data/CARGA_ENERGIA_2017.csv\n",
      "2024-09-27 00:37:40,119 [INFO] File ../data/CARGA_ENERGIA_2017.csv read successfully with 1460 rows.\n",
      "2024-09-27 00:37:40,121 [INFO] Loading data from CARGA_ENERGIA_2017.csv into the database.\n",
      "2024-09-27 00:37:40,122 [INFO] Deleting existing rows for CARGA_ENERGIA_2017.csv from the database.\n",
      "2024-09-27 00:37:40,131 [INFO] → Rows for CARGA_ENERGIA_2017.csv deleted successfully.\n",
      "2024-09-27 00:37:40,132 [INFO] Inserting new rows for CARGA_ENERGIA_2017.csv into the database.\n",
      "2024-09-27 00:37:41,358 [INFO] → Rows for CARGA_ENERGIA_2017.csv inserted successfully.\n",
      "2024-09-27 00:37:41,359 [INFO] Reading CSV file: ../data/CARGA_ENERGIA_2018.csv\n",
      "2024-09-27 00:37:41,365 [INFO] File ../data/CARGA_ENERGIA_2018.csv read successfully with 1460 rows.\n",
      "2024-09-27 00:37:41,367 [INFO] Loading data from CARGA_ENERGIA_2018.csv into the database.\n",
      "2024-09-27 00:37:41,368 [INFO] Deleting existing rows for CARGA_ENERGIA_2018.csv from the database.\n",
      "2024-09-27 00:37:41,377 [INFO] → Rows for CARGA_ENERGIA_2018.csv deleted successfully.\n",
      "2024-09-27 00:37:41,378 [INFO] Inserting new rows for CARGA_ENERGIA_2018.csv into the database.\n",
      "2024-09-27 00:37:42,509 [INFO] → Rows for CARGA_ENERGIA_2018.csv inserted successfully.\n",
      "2024-09-27 00:37:42,511 [INFO] Reading CSV file: ../data/CARGA_ENERGIA_2019.csv\n",
      "2024-09-27 00:37:42,517 [INFO] File ../data/CARGA_ENERGIA_2019.csv read successfully with 1460 rows.\n",
      "2024-09-27 00:37:42,519 [INFO] Loading data from CARGA_ENERGIA_2019.csv into the database.\n",
      "2024-09-27 00:37:42,520 [INFO] Deleting existing rows for CARGA_ENERGIA_2019.csv from the database.\n",
      "2024-09-27 00:37:42,530 [INFO] → Rows for CARGA_ENERGIA_2019.csv deleted successfully.\n",
      "2024-09-27 00:37:42,531 [INFO] Inserting new rows for CARGA_ENERGIA_2019.csv into the database.\n",
      "2024-09-27 00:37:43,643 [INFO] → Rows for CARGA_ENERGIA_2019.csv inserted successfully.\n",
      "2024-09-27 00:37:43,644 [INFO] Reading CSV file: ../data/CARGA_ENERGIA_2020.csv\n",
      "2024-09-27 00:37:43,650 [INFO] File ../data/CARGA_ENERGIA_2020.csv read successfully with 1464 rows.\n",
      "2024-09-27 00:37:43,651 [INFO] Loading data from CARGA_ENERGIA_2020.csv into the database.\n",
      "2024-09-27 00:37:43,652 [INFO] Deleting existing rows for CARGA_ENERGIA_2020.csv from the database.\n",
      "2024-09-27 00:37:43,668 [INFO] → Rows for CARGA_ENERGIA_2020.csv deleted successfully.\n",
      "2024-09-27 00:37:43,668 [INFO] Inserting new rows for CARGA_ENERGIA_2020.csv into the database.\n",
      "2024-09-27 00:37:44,819 [INFO] → Rows for CARGA_ENERGIA_2020.csv inserted successfully.\n",
      "2024-09-27 00:37:44,820 [INFO] Reading CSV file: ../data/CARGA_ENERGIA_2021.csv\n",
      "2024-09-27 00:37:44,825 [INFO] File ../data/CARGA_ENERGIA_2021.csv read successfully with 1460 rows.\n",
      "2024-09-27 00:37:44,827 [INFO] Loading data from CARGA_ENERGIA_2021.csv into the database.\n",
      "2024-09-27 00:37:44,827 [INFO] Deleting existing rows for CARGA_ENERGIA_2021.csv from the database.\n",
      "2024-09-27 00:37:44,835 [INFO] → Rows for CARGA_ENERGIA_2021.csv deleted successfully.\n",
      "2024-09-27 00:37:44,837 [INFO] Inserting new rows for CARGA_ENERGIA_2021.csv into the database.\n",
      "2024-09-27 00:37:45,985 [INFO] → Rows for CARGA_ENERGIA_2021.csv inserted successfully.\n",
      "2024-09-27 00:37:45,986 [INFO] Reading CSV file: ../data/CARGA_ENERGIA_2022.csv\n",
      "2024-09-27 00:37:45,991 [INFO] File ../data/CARGA_ENERGIA_2022.csv read successfully with 1460 rows.\n",
      "2024-09-27 00:37:45,993 [INFO] Loading data from CARGA_ENERGIA_2022.csv into the database.\n",
      "2024-09-27 00:37:45,994 [INFO] Deleting existing rows for CARGA_ENERGIA_2022.csv from the database.\n",
      "2024-09-27 00:37:46,003 [INFO] → Rows for CARGA_ENERGIA_2022.csv deleted successfully.\n",
      "2024-09-27 00:37:46,004 [INFO] Inserting new rows for CARGA_ENERGIA_2022.csv into the database.\n",
      "2024-09-27 00:37:47,147 [INFO] → Rows for CARGA_ENERGIA_2022.csv inserted successfully.\n",
      "2024-09-27 00:37:47,148 [INFO] Reading CSV file: ../data/CARGA_ENERGIA_2023.csv\n",
      "2024-09-27 00:37:47,153 [INFO] File ../data/CARGA_ENERGIA_2023.csv read successfully with 1460 rows.\n",
      "2024-09-27 00:37:47,155 [INFO] Loading data from CARGA_ENERGIA_2023.csv into the database.\n",
      "2024-09-27 00:37:47,156 [INFO] Deleting existing rows for CARGA_ENERGIA_2023.csv from the database.\n",
      "2024-09-27 00:37:47,165 [INFO] → Rows for CARGA_ENERGIA_2023.csv deleted successfully.\n",
      "2024-09-27 00:37:47,166 [INFO] Inserting new rows for CARGA_ENERGIA_2023.csv into the database.\n",
      "2024-09-27 00:37:48,260 [INFO] → Rows for CARGA_ENERGIA_2023.csv inserted successfully.\n",
      "2024-09-27 00:37:48,261 [INFO] Reading CSV file: ../data/CARGA_ENERGIA_2024.csv\n",
      "2024-09-27 00:37:48,265 [INFO] File ../data/CARGA_ENERGIA_2024.csv read successfully with 1076 rows.\n",
      "2024-09-27 00:37:48,267 [INFO] Loading data from CARGA_ENERGIA_2024.csv into the database.\n",
      "2024-09-27 00:37:48,268 [INFO] Deleting existing rows for CARGA_ENERGIA_2024.csv from the database.\n",
      "2024-09-27 00:37:48,278 [INFO] → Rows for CARGA_ENERGIA_2024.csv deleted successfully.\n",
      "2024-09-27 00:37:48,278 [INFO] Inserting new rows for CARGA_ENERGIA_2024.csv into the database.\n",
      "2024-09-27 00:37:49,122 [INFO] → Rows for CARGA_ENERGIA_2024.csv inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "\n",
    "# Function to read downloaded CSV files into pandas DataFrames\n",
    "def read_csv_file(file_path):\n",
    "    try:\n",
    "        logging.info(f\"Reading CSV file: {file_path}\")\n",
    "        df = pd.read_csv(file_path, sep=\";\", decimal=\",\")\n",
    "        logging.info(f\"File {file_path} read successfully with {len(df)} rows.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to delete existing rows for the file from the `carga_diaria` table\n",
    "def delete_existing_rows(input_file):\n",
    "    try:\n",
    "        logging.info(f\"Deleting existing rows for {input_file} from the database.\")\n",
    "        delete_query = \"DELETE FROM carga_diaria WHERE input_file = %s;\"\n",
    "        cursor.execute(delete_query, (input_file,))\n",
    "        db_connection.commit()\n",
    "        logging.info(f\"→ Rows for {input_file} deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to delete rows for {input_file}: {e}\")\n",
    "        db_connection.rollback()\n",
    "\n",
    "\n",
    "# Function to insert all rows from the DataFrame into the `carga_diaria` table\n",
    "def insert_file_to_db(df, input_file):\n",
    "    try:\n",
    "        logging.info(f\"Inserting new rows for {input_file} into the database.\")\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO carga_diaria (id_subsistema, nom_subsistema, din_instante, val_cargaenergiamwmed, Ano, input_file)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s);\n",
    "        \"\"\"\n",
    "        data_to_insert = [\n",
    "            (\n",
    "                row[\"id_subsistema\"],\n",
    "                row[\"nom_subsistema\"],\n",
    "                row[\"din_instante\"],\n",
    "                row[\"val_cargaenergiamwmed\"],\n",
    "                row[\"Ano\"],\n",
    "                input_file,\n",
    "            )\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "        cursor.executemany(insert_query, data_to_insert)\n",
    "        db_connection.commit()\n",
    "        logging.info(f\"→ Rows for {input_file} inserted successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to insert rows for {input_file}: {e}\")\n",
    "        db_connection.rollback()\n",
    "\n",
    "\n",
    "# Function to load data from a file into the database\n",
    "def load_file_to_db(df, input_file):\n",
    "    logging.info(f\"Loading data from {input_file} into the database.\")\n",
    "    # Step 1: Delete existing rows for this file\n",
    "    delete_existing_rows(input_file)\n",
    "    # Step 2: Insert the new data from the file\n",
    "    insert_file_to_db(df, input_file)\n",
    "\n",
    "\n",
    "# Main function to process all downloaded files and load them into the database\n",
    "def process_and_load_files(file_dir=\"data/\"):\n",
    "    logging.info(f\"Processing files in directory: {file_dir}\")\n",
    "    for file_name in os.listdir(file_dir):\n",
    "        file_path = os.path.join(file_dir, file_name)\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            df = read_csv_file(file_path)\n",
    "            if df is not None:\n",
    "                # Add the year based on the file name\n",
    "                year = int(file_name.split(\"_\")[-1].replace(\".csv\", \"\"))\n",
    "                df[\"Ano\"] = year\n",
    "                df[\"input_file\"] = file_name\n",
    "                # Load the data into the database (delete old rows, insert new ones)\n",
    "                load_file_to_db(df, file_name)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "process_and_load_files(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "current-flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
